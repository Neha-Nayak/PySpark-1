{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb6fb24",
   "metadata": {},
   "source": [
    "# Prediction of the performance of students in a particular student based on the subject code(taking last 2 digits into considration) ,Output = GRADE .....Using ML libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b875a19",
   "metadata": {},
   "source": [
    "### importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c061a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da9367",
   "metadata": {},
   "source": [
    "### creating Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e68400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:22:10 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.65.100 instead (on interface wlo1)\n",
      "22/04/14 18:22:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/media/bitmos/New%20Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/14 18:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30524856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.65.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe9b8e5c5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2cad6",
   "metadata": {},
   "source": [
    "### reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b66becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---+---+------+-----+-----+--------+\n",
      "|       USN|   SCODE|CIE|SEE|TOT|RESULT|GRADE| DATE|ExamType|\n",
      "+----------+--------+---+---+---+------+-----+-----+--------+\n",
      "|4JN18CS001| 18MAT11| 32| 41| 73|     P|    8|12019| Regular|\n",
      "|4JN18CS001| 18PHY12| 29| 47| 76|     P|    8|12019| Regular|\n",
      "|4JN18CS001| 18ELE13| 25| 43| 68|     P|    7|12019| Regular|\n",
      "|4JN18CS001| 18CIV14| 31| 26| 57|     P|    6|12019| Regular|\n",
      "|4JN18CS001|18EGDL15| 31| 56| 87|     P|    9|12019| Regular|\n",
      "|4JN18CS001|18PHYL16| 32| 40| 72|     P|    8|12019| Regular|\n",
      "|4JN18CS001|18ELEL17| 34| 52| 86|     P|    9|12019| Regular|\n",
      "|4JN18CS001| 18EGH18| 27| 32| 59|     P|    6|12019| Regular|\n",
      "|4JN18CS002| 18MAT11| 33| 51| 84|     P|    9|12019| Regular|\n",
      "|4JN18CS002| 18PHY12| 36| 45| 81|     P|    9|12019| Regular|\n",
      "|4JN18CS002| 18ELE13| 37| 31| 68|     P|    7|12019| Regular|\n",
      "|4JN18CS002| 18CIV14| 34| 29| 63|     P|    7|12019| Regular|\n",
      "|4JN18CS002|18EGDL15| 34| 55| 89|     P|    9|12019| Regular|\n",
      "|4JN18CS002|18PHYL16| 31| 44| 75|     P|    8|12019| Regular|\n",
      "|4JN18CS002|18ELEL17| 31| 53| 84|     P|    9|12019| Regular|\n",
      "|4JN18CS002| 18EGH18| 34| 38| 72|     P|    8|12019| Regular|\n",
      "|4JN18CS003| 18MAT11| 37| 44| 81|     P|    9|12019| Regular|\n",
      "|4JN18CS003| 18PHY12| 33| 40| 73|     P|    8|12019| Regular|\n",
      "|4JN18CS003| 18ELE13| 39| 42| 81|     P|    9|12019| Regular|\n",
      "|4JN18CS003| 18CIV14| 39| 33| 72|     P|    8|12019| Regular|\n",
      "+----------+--------+---+---+---+------+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Results_Data.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26064e84",
   "metadata": {},
   "source": [
    "### preprosessnig dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e86584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+\n",
      "|       USN|GRADE|SCODEFINAL|NEWDATE|\n",
      "+----------+-----+----------+-------+\n",
      "|4JN18CS001|    8|         1|     19|\n",
      "|4JN18CS001|    8|         2|     19|\n",
      "|4JN18CS001|    7|         3|     19|\n",
      "|4JN18CS001|    6|         4|     19|\n",
      "|4JN18CS001|    9|         5|     19|\n",
      "|4JN18CS001|    8|         6|     19|\n",
      "|4JN18CS001|    9|         7|     19|\n",
      "|4JN18CS001|    6|         8|     19|\n",
      "|4JN18CS002|    9|         1|     19|\n",
      "|4JN18CS002|    9|         2|     19|\n",
      "|4JN18CS002|    7|         3|     19|\n",
      "|4JN18CS002|    7|         4|     19|\n",
      "|4JN18CS002|    9|         5|     19|\n",
      "|4JN18CS002|    8|         6|     19|\n",
      "|4JN18CS002|    9|         7|     19|\n",
      "|4JN18CS002|    8|         8|     19|\n",
      "|4JN18CS003|    9|         1|     19|\n",
      "|4JN18CS003|    8|         2|     19|\n",
      "|4JN18CS003|    9|         3|     19|\n",
      "|4JN18CS003|    8|         4|     19|\n",
      "+----------+-----+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df2_view\")\n",
    "\n",
    "dffinal = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT USN,GRADE,\n",
    "    (CASE\n",
    "    WHEN SCODE LIKE '%1' THEN 1\n",
    "    WHEN SCODE LIKE '%2' THEN 2\n",
    "    WHEN SCODE LIKE '%3' THEN 3\n",
    "    WHEN SCODE LIKE '%4' THEN 4\n",
    "    WHEN SCODE LIKE '%5' THEN 5\n",
    "    WHEN SCODE LIKE '%6' THEN 6\n",
    "    WHEN SCODE LIKE '%7' THEN 7\n",
    "    WHEN SCODE LIKE '%8' THEN 8\n",
    "    WHEN SCODE LIKE '%9' THEN 9\n",
    "    END) AS SCODEFINAL,\n",
    "    (CASE\n",
    "    WHEN DATE LIKE '%19' THEN 19\n",
    "    WHEN DATE LIKE '%20' THEN 20\n",
    "    END) AS NEWDATE\n",
    "    \n",
    "    FROM df2_view\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "dffinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d04b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+----------+\n",
      "|       USN|GRADE|SCODEFINAL|NEWDATE|EncodedUSN|\n",
      "+----------+-----+----------+-------+----------+\n",
      "|4JN18CS001|    8|         1|     19|     242.0|\n",
      "|4JN18CS001|    8|         2|     19|     242.0|\n",
      "|4JN18CS001|    7|         3|     19|     242.0|\n",
      "|4JN18CS001|    6|         4|     19|     242.0|\n",
      "|4JN18CS001|    9|         5|     19|     242.0|\n",
      "|4JN18CS001|    8|         6|     19|     242.0|\n",
      "|4JN18CS001|    9|         7|     19|     242.0|\n",
      "|4JN18CS001|    6|         8|     19|     242.0|\n",
      "|4JN18CS002|    9|         1|     19|     243.0|\n",
      "|4JN18CS002|    9|         2|     19|     243.0|\n",
      "|4JN18CS002|    7|         3|     19|     243.0|\n",
      "|4JN18CS002|    7|         4|     19|     243.0|\n",
      "|4JN18CS002|    9|         5|     19|     243.0|\n",
      "|4JN18CS002|    8|         6|     19|     243.0|\n",
      "|4JN18CS002|    9|         7|     19|     243.0|\n",
      "|4JN18CS002|    8|         8|     19|     243.0|\n",
      "|4JN18CS003|    9|         1|     19|     244.0|\n",
      "|4JN18CS003|    8|         2|     19|     244.0|\n",
      "|4JN18CS003|    9|         3|     19|     244.0|\n",
      "|4JN18CS003|    8|         4|     19|     244.0|\n",
      "+----------+-----+----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qualification_indexer = StringIndexer(inputCol=\"USN\", outputCol=\"EncodedUSN\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "df1 = qualification_indexer.fit(dffinal).transform(dffinal)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9543d56f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[USN: string, GRADE: int, SCODEFINAL: int, NEWDATE: int]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d099515f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+----------+----------------+\n",
      "|       USN|GRADE|SCODEFINAL|NEWDATE|EncodedUSN|             NEW|\n",
      "+----------+-----+----------+-------+----------+----------------+\n",
      "|4JN18CS001|    8|         1|     19|     242.0|[242.0,19.0,1.0]|\n",
      "|4JN18CS001|    8|         2|     19|     242.0|[242.0,19.0,2.0]|\n",
      "|4JN18CS001|    7|         3|     19|     242.0|[242.0,19.0,3.0]|\n",
      "|4JN18CS001|    6|         4|     19|     242.0|[242.0,19.0,4.0]|\n",
      "|4JN18CS001|    9|         5|     19|     242.0|[242.0,19.0,5.0]|\n",
      "|4JN18CS001|    8|         6|     19|     242.0|[242.0,19.0,6.0]|\n",
      "|4JN18CS001|    9|         7|     19|     242.0|[242.0,19.0,7.0]|\n",
      "|4JN18CS001|    6|         8|     19|     242.0|[242.0,19.0,8.0]|\n",
      "|4JN18CS002|    9|         1|     19|     243.0|[243.0,19.0,1.0]|\n",
      "|4JN18CS002|    9|         2|     19|     243.0|[243.0,19.0,2.0]|\n",
      "|4JN18CS002|    7|         3|     19|     243.0|[243.0,19.0,3.0]|\n",
      "|4JN18CS002|    7|         4|     19|     243.0|[243.0,19.0,4.0]|\n",
      "|4JN18CS002|    9|         5|     19|     243.0|[243.0,19.0,5.0]|\n",
      "|4JN18CS002|    8|         6|     19|     243.0|[243.0,19.0,6.0]|\n",
      "|4JN18CS002|    9|         7|     19|     243.0|[243.0,19.0,7.0]|\n",
      "|4JN18CS002|    8|         8|     19|     243.0|[243.0,19.0,8.0]|\n",
      "|4JN18CS003|    9|         1|     19|     244.0|[244.0,19.0,1.0]|\n",
      "|4JN18CS003|    8|         2|     19|     244.0|[244.0,19.0,2.0]|\n",
      "|4JN18CS003|    9|         3|     19|     244.0|[244.0,19.0,3.0]|\n",
      "|4JN18CS003|    8|         4|     19|     244.0|[244.0,19.0,4.0]|\n",
      "+----------+-----+----------+-------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=['EncodedUSN','NEWDATE','SCODEFINAL'],outputCol=\"NEW\")\n",
    "output=featureassembler.transform(df1)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea38b87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+----------+----------------+\n",
      "|       USN|GRADE|SCODEFINAL|NEWDATE|EncodedUSN|             NEW|\n",
      "+----------+-----+----------+-------+----------+----------------+\n",
      "|4JN18CS001|    8|         1|     19|     242.0|[242.0,19.0,1.0]|\n",
      "|4JN18CS001|    8|         2|     19|     242.0|[242.0,19.0,2.0]|\n",
      "|4JN18CS001|    7|         3|     19|     242.0|[242.0,19.0,3.0]|\n",
      "|4JN18CS001|    6|         4|     19|     242.0|[242.0,19.0,4.0]|\n",
      "|4JN18CS001|    9|         5|     19|     242.0|[242.0,19.0,5.0]|\n",
      "|4JN18CS001|    8|         6|     19|     242.0|[242.0,19.0,6.0]|\n",
      "|4JN18CS001|    9|         7|     19|     242.0|[242.0,19.0,7.0]|\n",
      "|4JN18CS001|    6|         8|     19|     242.0|[242.0,19.0,8.0]|\n",
      "|4JN18CS002|    9|         1|     19|     243.0|[243.0,19.0,1.0]|\n",
      "|4JN18CS002|    9|         2|     19|     243.0|[243.0,19.0,2.0]|\n",
      "|4JN18CS002|    7|         3|     19|     243.0|[243.0,19.0,3.0]|\n",
      "|4JN18CS002|    7|         4|     19|     243.0|[243.0,19.0,4.0]|\n",
      "|4JN18CS002|    9|         5|     19|     243.0|[243.0,19.0,5.0]|\n",
      "|4JN18CS002|    8|         6|     19|     243.0|[243.0,19.0,6.0]|\n",
      "|4JN18CS002|    9|         7|     19|     243.0|[243.0,19.0,7.0]|\n",
      "|4JN18CS002|    8|         8|     19|     243.0|[243.0,19.0,8.0]|\n",
      "|4JN18CS003|    9|         1|     19|     244.0|[244.0,19.0,1.0]|\n",
      "|4JN18CS003|    8|         2|     19|     244.0|[244.0,19.0,2.0]|\n",
      "|4JN18CS003|    9|         3|     19|     244.0|[244.0,19.0,3.0]|\n",
      "|4JN18CS003|    8|         4|     19|     244.0|[244.0,19.0,4.0]|\n",
      "+----------+-----+----------+-------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = output.limit(10000)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bb5d8dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column USN must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[1;32m      3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(\n\u001b[1;32m      4\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSN\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \n\u001b[1;32m      5\u001b[0m     outputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewusn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m encoded_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column USN must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"USN\"],  \n",
    "    outputCols=[\"newusn\"]\n",
    ")\n",
    "encoded_df = encoder.fit(df2)\n",
    "encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a8b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|             NEW|GRADE|\n",
      "+----------------+-----+\n",
      "|[242.0,19.0,1.0]|    8|\n",
      "|[242.0,19.0,2.0]|    8|\n",
      "|[242.0,19.0,3.0]|    7|\n",
      "|[242.0,19.0,4.0]|    6|\n",
      "|[242.0,19.0,5.0]|    9|\n",
      "|[242.0,19.0,6.0]|    8|\n",
      "|[242.0,19.0,7.0]|    9|\n",
      "|[242.0,19.0,8.0]|    6|\n",
      "|[243.0,19.0,1.0]|    9|\n",
      "|[243.0,19.0,2.0]|    9|\n",
      "|[243.0,19.0,3.0]|    7|\n",
      "|[243.0,19.0,4.0]|    7|\n",
      "|[243.0,19.0,5.0]|    9|\n",
      "|[243.0,19.0,6.0]|    8|\n",
      "|[243.0,19.0,7.0]|    9|\n",
      "|[243.0,19.0,8.0]|    8|\n",
      "|[244.0,19.0,1.0]|    9|\n",
      "|[244.0,19.0,2.0]|    8|\n",
      "|[244.0,19.0,3.0]|    9|\n",
      "|[244.0,19.0,4.0]|    8|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data = df2.select(['NEW','GRADE'])\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1ab72",
   "metadata": {},
   "source": [
    "### ML training starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286bd3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 8987\n",
      "Test Dataset Count: 1013\n"
     ]
    }
   ],
   "source": [
    "train, test = finalized_data.randomSplit([0.9, 0.1])\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a94fdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'NEW', labelCol = 'GRADE',maxBins=720)\n",
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95bb6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+\n",
      "|            NEW|GRADE|prediction|\n",
      "+---------------+-----+----------+\n",
      "| [0.0,19.0,1.0]|    0|       0.0|\n",
      "| [0.0,19.0,2.0]|    6|       7.0|\n",
      "| [1.0,19.0,3.0]|    6|       6.0|\n",
      "| [2.0,19.0,1.0]|    0|       7.0|\n",
      "| [2.0,19.0,3.0]|    0|       6.0|\n",
      "| [3.0,19.0,1.0]|    0|       0.0|\n",
      "| [4.0,19.0,1.0]|    4|       8.0|\n",
      "| [4.0,19.0,1.0]|    6|       8.0|\n",
      "| [4.0,19.0,3.0]|    0|       6.0|\n",
      "| [4.0,19.0,3.0]|    6|       6.0|\n",
      "| [4.0,19.0,3.0]|    6|       6.0|\n",
      "| [4.0,19.0,5.0]|    4|       6.0|\n",
      "| [5.0,19.0,3.0]|    6|       6.0|\n",
      "| [5.0,19.0,9.0]|    0|       0.0|\n",
      "| [6.0,19.0,3.0]|    5|       6.0|\n",
      "| [7.0,19.0,1.0]|    0|       7.0|\n",
      "| [7.0,19.0,6.0]|    5|       6.0|\n",
      "| [8.0,19.0,1.0]|    0|       6.0|\n",
      "| [8.0,19.0,1.0]|    4|       6.0|\n",
      "| [9.0,19.0,1.0]|    0|       7.0|\n",
      "| [9.0,19.0,1.0]|    5|       7.0|\n",
      "| [9.0,19.0,9.0]|    0|       0.0|\n",
      "| [9.0,19.0,9.0]|    4|       0.0|\n",
      "|[11.0,19.0,4.0]|    4|       6.0|\n",
      "|[11.0,19.0,7.0]|    8|      10.0|\n",
      "+---------------+-----+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(test)\n",
    "predictions.select(['NEW','GRADE','prediction']).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f985d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|GRADE|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    6|       7.0|\n",
      "|    6|       6.0|\n",
      "|    0|       7.0|\n",
      "|    0|       6.0|\n",
      "|    0|       0.0|\n",
      "|    4|       8.0|\n",
      "|    6|       8.0|\n",
      "|    0|       6.0|\n",
      "|    6|       6.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"GRADE\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62d70223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+----+\n",
      "|       USN|   SCODE|GRADE|2021|\n",
      "+----------+--------+-----+----+\n",
      "|4JN18CS001| 18MAT11|    8|2021|\n",
      "|4JN18CS001| 18PHY12|    8|2021|\n",
      "|4JN18CS001| 18ELE13|    7|2021|\n",
      "|4JN18CS001| 18CIV14|    6|2021|\n",
      "|4JN18CS001|18EGDL15|    9|2021|\n",
      "|4JN18CS001|18PHYL16|    8|2021|\n",
      "|4JN18CS001|18ELEL17|    9|2021|\n",
      "|4JN18CS001| 18EGH18|    6|2021|\n",
      "|4JN18CS002| 18MAT11|    9|2021|\n",
      "|4JN18CS002| 18PHY12|    9|2021|\n",
      "|4JN18CS002| 18ELE13|    7|2021|\n",
      "|4JN18CS002| 18CIV14|    7|2021|\n",
      "|4JN18CS002|18EGDL15|    9|2021|\n",
      "|4JN18CS002|18PHYL16|    8|2021|\n",
      "|4JN18CS002|18ELEL17|    9|2021|\n",
      "|4JN18CS002| 18EGH18|    8|2021|\n",
      "|4JN18CS003| 18MAT11|    9|2021|\n",
      "|4JN18CS003| 18PHY12|    8|2021|\n",
      "|4JN18CS003| 18ELE13|    9|2021|\n",
      "|4JN18CS003| 18CIV14|    8|2021|\n",
      "+----------+--------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest = spark.read.csv('new.csv',header=True,inferSchema=True)\n",
    "dftest.show()\n",
    "dftest = dftest.limit(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c4e8a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+----+----------+\n",
      "|       USN|   SCODE|GRADE|2021|EncodedUSN|\n",
      "+----------+--------+-----+----+----------+\n",
      "|4JN18CS001| 18MAT11|    8|2021|     242.0|\n",
      "|4JN18CS001| 18PHY12|    8|2021|     242.0|\n",
      "|4JN18CS001| 18ELE13|    7|2021|     242.0|\n",
      "|4JN18CS001| 18CIV14|    6|2021|     242.0|\n",
      "|4JN18CS001|18EGDL15|    9|2021|     242.0|\n",
      "|4JN18CS001|18PHYL16|    8|2021|     242.0|\n",
      "|4JN18CS001|18ELEL17|    9|2021|     242.0|\n",
      "|4JN18CS001| 18EGH18|    6|2021|     242.0|\n",
      "|4JN18CS002| 18MAT11|    9|2021|     243.0|\n",
      "|4JN18CS002| 18PHY12|    9|2021|     243.0|\n",
      "|4JN18CS002| 18ELE13|    7|2021|     243.0|\n",
      "|4JN18CS002| 18CIV14|    7|2021|     243.0|\n",
      "|4JN18CS002|18EGDL15|    9|2021|     243.0|\n",
      "|4JN18CS002|18PHYL16|    8|2021|     243.0|\n",
      "|4JN18CS002|18ELEL17|    9|2021|     243.0|\n",
      "|4JN18CS002| 18EGH18|    8|2021|     243.0|\n",
      "|4JN18CS003| 18MAT11|    9|2021|     244.0|\n",
      "|4JN18CS003| 18PHY12|    8|2021|     244.0|\n",
      "|4JN18CS003| 18ELE13|    9|2021|     244.0|\n",
      "|4JN18CS003| 18CIV14|    8|2021|     244.0|\n",
      "+----------+--------+-----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qualification_indexer = StringIndexer(inputCol=\"USN\", outputCol=\"EncodedUSN\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "dftest1 = qualification_indexer.fit(dftest).transform(dftest)\n",
    "dftest1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "382b61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+----------+-------+\n",
      "|       USN|EncodedUSN|GRADE|SCODEFINAL|NEWDATE|\n",
      "+----------+----------+-----+----------+-------+\n",
      "|4JN18CS001|     242.0|    8|         1|     21|\n",
      "|4JN18CS001|     242.0|    8|         2|     21|\n",
      "|4JN18CS001|     242.0|    7|         3|     21|\n",
      "|4JN18CS001|     242.0|    6|         4|     21|\n",
      "|4JN18CS001|     242.0|    9|         5|     21|\n",
      "|4JN18CS001|     242.0|    8|         6|     21|\n",
      "|4JN18CS001|     242.0|    9|         7|     21|\n",
      "|4JN18CS001|     242.0|    6|         8|     21|\n",
      "|4JN18CS002|     243.0|    9|         1|     21|\n",
      "|4JN18CS002|     243.0|    9|         2|     21|\n",
      "|4JN18CS002|     243.0|    7|         3|     21|\n",
      "|4JN18CS002|     243.0|    7|         4|     21|\n",
      "|4JN18CS002|     243.0|    9|         5|     21|\n",
      "|4JN18CS002|     243.0|    8|         6|     21|\n",
      "|4JN18CS002|     243.0|    9|         7|     21|\n",
      "|4JN18CS002|     243.0|    8|         8|     21|\n",
      "|4JN18CS003|     244.0|    9|         1|     21|\n",
      "|4JN18CS003|     244.0|    8|         2|     21|\n",
      "|4JN18CS003|     244.0|    9|         3|     21|\n",
      "|4JN18CS003|     244.0|    8|         4|     21|\n",
      "+----------+----------+-----+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest1.createOrReplaceTempView(\"df3_view\")\n",
    "\n",
    "dffinal2 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT USN,EncodedUSN,GRADE,\n",
    "    (CASE\n",
    "    WHEN SCODE LIKE '%1' THEN 1\n",
    "    WHEN SCODE LIKE '%2' THEN 2\n",
    "    WHEN SCODE LIKE '%3' THEN 3\n",
    "    WHEN SCODE LIKE '%4' THEN 4\n",
    "    WHEN SCODE LIKE '%5' THEN 5\n",
    "    WHEN SCODE LIKE '%6' THEN 6\n",
    "    WHEN SCODE LIKE '%7' THEN 7\n",
    "    WHEN SCODE LIKE '%8' THEN 8\n",
    "    WHEN SCODE LIKE '%9' THEN 9\n",
    "    END) AS SCODEFINAL,\n",
    "    (CASE\n",
    "    WHEN 2021 LIKE '%21' THEN 21\n",
    "    END) AS NEWDATE\n",
    "    \n",
    "    FROM df3_view\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "dffinal2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e961bacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column USN is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m featureassembler\u001b[38;5;241m=\u001b[39mVectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncodedUSN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEWDATE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCODEFINAL\u001b[39m\u001b[38;5;124m'\u001b[39m],outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m output1\u001b[38;5;241m=\u001b[39m\u001b[43mfeatureassembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdffinal2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m output1\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/wrapper.py:350\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type string of column USN is not supported."
     ]
    }
   ],
   "source": [
    "featureassembler=VectorAssembler(inputCols=['USN','EncodedUSN','NEWDATE','SCODEFINAL'],outputCol=\"NEW\")\n",
    "output1=featureassembler.transform(dffinal2)\n",
    "output1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d31c340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfinal = output1.select('NEW','GRADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cb0e732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('EncodedUSN')==\"242.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49553bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "StringIndexer(inputCols=[\"USN\"], outputCols=[\"newUSN1\"])\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"newUSN1\"],  \n",
    "    outputCols=[\"newUSN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c11a1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+------+\n",
      "|       USN|GRADE|SCODEFINAL|NEWDATE|newUSN|\n",
      "+----------+-----+----------+-------+------+\n",
      "|4JN18CS001|    8|         1|     19| 242.0|\n",
      "|4JN18CS001|    8|         2|     19| 242.0|\n",
      "|4JN18CS001|    7|         3|     19| 242.0|\n",
      "|4JN18CS001|    6|         4|     19| 242.0|\n",
      "|4JN18CS001|    9|         5|     19| 242.0|\n",
      "|4JN18CS001|    8|         6|     19| 242.0|\n",
      "|4JN18CS001|    9|         7|     19| 242.0|\n",
      "|4JN18CS001|    6|         8|     19| 242.0|\n",
      "|4JN18CS002|    9|         1|     19| 243.0|\n",
      "|4JN18CS002|    9|         2|     19| 243.0|\n",
      "|4JN18CS002|    7|         3|     19| 243.0|\n",
      "|4JN18CS002|    7|         4|     19| 243.0|\n",
      "|4JN18CS002|    9|         5|     19| 243.0|\n",
      "|4JN18CS002|    8|         6|     19| 243.0|\n",
      "|4JN18CS002|    9|         7|     19| 243.0|\n",
      "|4JN18CS002|    8|         8|     19| 243.0|\n",
      "|4JN18CS003|    9|         1|     19| 244.0|\n",
      "|4JN18CS003|    8|         2|     19| 244.0|\n",
      "|4JN18CS003|    9|         3|     19| 244.0|\n",
      "|4JN18CS003|    8|         4|     19| 244.0|\n",
      "+----------+-----+----------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"USN\", outputCol=\"newUSN\").fit(dffinal)\n",
    "indexed_df = indexer.transform(dffinal)\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40aa32c",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51fd3acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|GRADE|prediction|\n",
      "+-----+----------+\n",
      "|    8|       6.0|\n",
      "|    8|       6.0|\n",
      "|    7|       6.0|\n",
      "|    6|       6.0|\n",
      "|    9|       6.0|\n",
      "|    8|       6.0|\n",
      "|    9|      10.0|\n",
      "|    6|      10.0|\n",
      "|    9|       6.0|\n",
      "|    9|       6.0|\n",
      "|    7|       6.0|\n",
      "|    7|       6.0|\n",
      "|    9|       6.0|\n",
      "|    8|       7.0|\n",
      "|    9|      10.0|\n",
      "|    8|      10.0|\n",
      "|    9|       6.0|\n",
      "|    8|       6.0|\n",
      "|    9|       6.0|\n",
      "|    8|       6.0|\n",
      "|    6|       6.0|\n",
      "|    6|       6.0|\n",
      "|    9|      10.0|\n",
      "|    8|      10.0|\n",
      "|    5|       6.0|\n",
      "+-----+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(testfinal)\n",
    "predictions.select(['GRADE','prediction']).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deb8886a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.first()['prediction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01fdf20b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o473.save.\n: java.io.IOException: Path -f/rfModel already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_rfc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(path)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrfModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrfModel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/util.py:226\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/ml/util.py:177\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/media/bitmos/New Volume/Coders_Club/PySpark/PySpark/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o473.save.\n: java.io.IOException: Path -f/rfModel already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "path = 'model_rfc'\n",
    "os.mkdir(path)\n",
    "rfModel.save(os.path.join(sys.argv[1], 'rfModel'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
